# LiteLLM Configuration
# Copy this file to .env and fill in your values

# Default model to use (examples below)
# OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
# Anthropic: claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022, claude-opus-4-5-20251101
# Azure: azure/gpt-4o, azure/gpt-35-turbo
# Other: gemini/gemini-pro, bedrock/anthropic.claude-v2, etc.
LITELLM_DEFAULT_MODEL=gpt-4o-mini

# Optional: Comma-separated list of models to escalate to on validation failures
# Example: gpt-4o-mini,gpt-4o (tries cheap model first, escalates to expensive on failure)
LITELLM_ESCALATE_MODELS=

# Optional: If using LiteLLM proxy (https://docs.litellm.ai/docs/proxy/quick_start)
# NOTE: You DON'T need to run a LiteLLM proxy for this demo!
# We use LiteLLM as a Python library, not as a separate service.
# Only set these if you're running a separate LiteLLM proxy server.
LITELLM_API_BASE=
LITELLM_API_KEY=

# Provider API keys (LiteLLM reads these automatically)
# OpenAI
OPENAI_API_KEY=your-openai-key-here

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-key-here

# Azure OpenAI
# AZURE_API_KEY=
# AZURE_API_BASE=
# AZURE_API_VERSION=

# Other providers (uncomment as needed)
# COHERE_API_KEY=
# AI21_API_KEY=
# REPLICATE_API_KEY=
# HUGGINGFACE_API_KEY=
